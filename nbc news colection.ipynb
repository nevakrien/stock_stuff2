{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a0b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import modin \n",
    "import numpy as np \n",
    "import requests as rq \n",
    "import bs4\n",
    "\n",
    "#bullshit imports for that script\n",
    "from time import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7001774",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list=[r\"https://www.nbcnews.com/business\"]\n",
    "warn=lambda x: print(\"WARNING:\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b70a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: 1; Frequency: 0.040836118609441474 requests/s\n",
      "Request: 1; Frequency: 0.04244809035035484 requests/s\n",
      "<class 'AttributeError'>\n",
      "Request: 2; Frequency: 0.04188366425162864 requests/s\n",
      "<class 'AttributeError'>\n",
      "Request: 3; Frequency: 0.040691095175808135 requests/s\n",
      "<class 'AttributeError'>\n"
     ]
    }
   ],
   "source": [
    "## Creating a loop to scrape from all pages\n",
    "\n",
    "news_title = []\n",
    "news_source = []\n",
    "news_link = []\n",
    "\n",
    "#pages = [str(i) for i in range(1,371)]\n",
    "\n",
    "reqs = 0\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for url in url_list:\n",
    "    \n",
    "    full_url = 'https://web.archive.org/web/'+url\n",
    "    \n",
    "    #open page\n",
    "    try:\n",
    "        pg = rq.get(full_url).text\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print('Error: {}'.format(e))\n",
    "    except urllib.error.URLError as e:\n",
    "        print('Error: {}'.format(e.reason))\n",
    "        \n",
    "    sleep(randint(10,20))\n",
    "    reqs +=1\n",
    "    \n",
    "    # Calculate elapsed time between requests\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(reqs,reqs/elapsed_time))\n",
    "    \n",
    "\n",
    "    \n",
    "    #Break once the max pages is reached\n",
    "    if reqs > len(url_list):\n",
    "        warn('No. of requests was greater than expected')\n",
    "        break\n",
    "        \n",
    "    # parse html using beautifulsoup and store in soup\n",
    "    soup = bs(pg,'html.parser')\n",
    "    \n",
    "    #find all news containers\n",
    "    articles = soup.find_all('article')\n",
    "    \n",
    "    # parse through news containers to get info\n",
    "    for article in articles:\n",
    "        try:\n",
    "            \n",
    "            if article != None:\n",
    "                #title and link\n",
    "                if article.find_all('h2') != None:\n",
    "                    #get news title\n",
    "                    title = article.find_all('h2')[1].a.text \n",
    "                    #get individual news article link\n",
    "                    link = article.find_all('h2')[1].a['href'] \n",
    "                else:\n",
    "                    title = 'N/A'\n",
    "                    link = 'N/A'\n",
    "                # source\n",
    "                source = 'NBC News'\n",
    "\n",
    "            # Append data elements to lists\n",
    "            news_title.append(title)\n",
    "            news_source.append(source)\n",
    "            news_link.append(link)\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(e)\n",
    " \n",
    " ## Creating a loop to scrape summary from links\n",
    "\n",
    "news_summary = []\n",
    "summ_link = []\n",
    "\n",
    "reqs = 0\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for url in news_link:\n",
    "    \n",
    "    #open page\n",
    "    try:\n",
    "        pg = rq.get(url).text\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print('Error: {}'.format(e))\n",
    "    except urllib.error.URLError as e:\n",
    "        print('Error: {}'.format(e.reason))\n",
    "        \n",
    "    sleep(randint(10,20))\n",
    "    reqs +=1\n",
    "    \n",
    "    # Calculate elapsed time between requests\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(reqs,reqs/elapsed_time))\n",
    "    \n",
    "\n",
    "    \n",
    "    #Break once the max pages is reached\n",
    "    if reqs > len(news_link):\n",
    "        warn('No. of requests was greater than expected')\n",
    "        break\n",
    "        \n",
    "    # parse html using beautifulsoup and store in soup\n",
    "    soup = bs(pg,'html.parser')\n",
    "    \n",
    "    #find all news containers\n",
    "    article = soup.find('div',attrs={'class':'article container___2EGEI'})\n",
    "    try:\n",
    "        if article.div != None:\n",
    "            summ = article.div.text\n",
    "        else:\n",
    "            summ = 'N/A'\n",
    "        \n",
    "        news_summary.append(summ)\n",
    "        summ_link.append(url)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe410974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f65d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
